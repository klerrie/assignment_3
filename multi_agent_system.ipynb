{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Agent System for Customer Support Routing\n",
    "\n",
    "A production-ready multi-agent orchestration system that intelligently routes customer inquiries to specialized RAG agents.\n",
    "\n",
    "## System Architecture\n",
    "\n",
    "1. **Orchestrator**: Classifies user intent (HR, Tech, Finance)\n",
    "2. **Specialized RAG Agents**: Domain-specific agents with separate knowledge bases\n",
    "3. **Langfuse Integration**: Full observability and tracing\n",
    "4. **Evaluator Agent**: Automated quality scoring (Bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ .env file found\n",
      "âœ“ Setup complete\n",
      "âœ“ Langfuse: Connected\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain 1.0+ imports\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Langfuse for observability\n",
    "from langfuse import Langfuse, observe\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify .env file exists\n",
    "env_file = Path(\".env\")\n",
    "if not env_file.exists():\n",
    "    print(\"âš  WARNING: .env file not found!\")\n",
    "    print(\"   Please create a .env file with your API keys.\")\n",
    "    print(\"   See README.md for instructions.\")\n",
    "else:\n",
    "    print(\"âœ“ .env file found\")\n",
    "\n",
    "# Initialize Langfuse\n",
    "langfuse = Langfuse(\n",
    "    public_key=os.getenv(\"LANGFUSE_PUBLIC_KEY\"),\n",
    "    secret_key=os.getenv(\"LANGFUSE_SECRET_KEY\"),\n",
    "    host=os.getenv(\"LANGFUSE_HOST\", \"https://cloud.langfuse.com\")\n",
    ")\n",
    "\n",
    "print(\"âœ“ Setup complete\")\n",
    "try:\n",
    "    print(f\"âœ“ Langfuse: {'Connected' if langfuse.auth_check() else 'Not connected'}\")\n",
    "except:\n",
    "    print(\"âš  Langfuse connection issue (system will still work)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Models & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ API key loaded (length: 73 characters)\n",
      "âœ“ Models initialized\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BASE_DIR = Path(\".\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "CHROMA_DIR = BASE_DIR / \"chroma_db\"\n",
    "\n",
    "# Validate API key is loaded (must run Cell 2 first!)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\n",
    "        \"ERROR: OPENAI_API_KEY not found!\\n\\n\"\n",
    "        \"Please:\\n\"\n",
    "        \"1. Create a .env file in the project root\\n\"\n",
    "        \"2. Add: OPENAI_API_KEY=your-openrouter-api-key-here\\n\"\n",
    "        \"3. Make sure you ran Cell 2 (Setup & Configuration) first\\n\"\n",
    "        \"4. Get your API key from: https://openrouter.ai\"\n",
    "    )\n",
    "\n",
    "print(f\"âœ“ API key loaded (length: {len(api_key)} characters)\")\n",
    "\n",
    "# Initialize LLM (via OpenRouter)\n",
    "llm = ChatOpenAI(\n",
    "    model=\"openai/gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=api_key,\n",
    "    openai_api_base=\"https://openrouter.ai/api/v1\"\n",
    ")\n",
    "\n",
    "# Text splitter configuration\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "print(\"âœ“ Models initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Documents & Create Vector Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 10 documents from hr\n",
      "âœ“ Loaded 10 documents from tech\n",
      "âœ“ Loaded 10 documents from finance\n",
      "\n",
      "ðŸ“Š Summary: HR=10, Tech=10, Finance=10\n"
     ]
    }
   ],
   "source": [
    "@observe()\n",
    "def load_domain_documents(domain: str) -> List[Document]:\n",
    "    \"\"\"Load all documents from a domain directory.\"\"\"\n",
    "    domain_path = DATA_DIR / f\"{domain}_docs\"\n",
    "    \n",
    "    if not domain_path.exists():\n",
    "        print(f\"âš  {domain_path} not found\")\n",
    "        return []\n",
    "    \n",
    "    loader = DirectoryLoader(\n",
    "        str(domain_path),\n",
    "        glob=\"**/*.txt\",\n",
    "        loader_cls=TextLoader\n",
    "    )\n",
    "    \n",
    "    documents = loader.load()\n",
    "    print(f\"âœ“ Loaded {len(documents)} documents from {domain}\")\n",
    "    return documents\n",
    "\n",
    "# Load documents for all domains\n",
    "hr_docs = load_domain_documents(\"hr\")\n",
    "tech_docs = load_domain_documents(\"tech\")\n",
    "finance_docs = load_domain_documents(\"finance\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Summary: HR={len(hr_docs)}, Tech={len(tech_docs)}, Finance={len(finance_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Created 20 chunks for hr\n",
      "âœ“ Created 24 chunks for tech\n",
      "âœ“ Created 29 chunks for finance\n",
      "\n",
      "âœ“ All vector stores created\n"
     ]
    }
   ],
   "source": [
    "@observe()\n",
    "def create_vector_store(documents: List[Document], domain: str) -> Chroma:\n",
    "    \"\"\"Create and persist a vector store for a domain.\"\"\"\n",
    "    if not documents:\n",
    "        raise ValueError(f\"No documents for {domain}\")\n",
    "    \n",
    "    # Split into chunks\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"âœ“ Created {len(chunks)} chunks for {domain}\")\n",
    "    \n",
    "    # Create vector store\n",
    "    persist_path = str(CHROMA_DIR / domain)\n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_path\n",
    "    )\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "# Create vector stores\n",
    "hr_vector_store = create_vector_store(hr_docs, \"hr\")\n",
    "tech_vector_store = create_vector_store(tech_docs, \"tech\")\n",
    "finance_vector_store = create_vector_store(finance_docs, \"finance\")\n",
    "\n",
    "print(\"\\nâœ“ All vector stores created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Specialized RAG Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ All RAG agents created\n"
     ]
    }
   ],
   "source": [
    "# Domain-specific prompt templates\n",
    "PROMPTS = {\n",
    "    \"HR\": \"\"\"You are a helpful HR assistant. Answer questions based on company HR documentation.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a clear, accurate answer based on the documentation.\n",
    "Answer:\"\"\",\n",
    "    \n",
    "    \"Tech\": \"\"\"You are a helpful IT support assistant. Answer technical questions based on company IT documentation.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a clear technical answer with specific steps when relevant.\n",
    "Answer:\"\"\",\n",
    "    \n",
    "    \"Finance\": \"\"\"You are a helpful finance assistant. Answer financial questions based on company finance documentation.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Provide a precise financial answer with accurate numbers and procedures.\n",
    "Answer:\"\"\"\n",
    "}\n",
    "\n",
    "@observe()\n",
    "def create_rag_chain(vector_store: Chroma, domain: str):\n",
    "    \"\"\"Create a RAG chain using LCEL for a domain.\"\"\"\n",
    "    # Create retriever\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 4})\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = PromptTemplate.from_template(PROMPTS[domain])\n",
    "    \n",
    "    # Format documents function\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    # Build LCEL chain\n",
    "    chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return chain, retriever\n",
    "\n",
    "# Create RAG chains for each domain\n",
    "hr_chain, hr_retriever = create_rag_chain(hr_vector_store, \"HR\")\n",
    "tech_chain, tech_retriever = create_rag_chain(tech_vector_store, \"Tech\")\n",
    "finance_chain, finance_retriever = create_rag_chain(finance_vector_store, \"Finance\")\n",
    "\n",
    "print(\"âœ“ All RAG agents created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Orchestrator: Intent Classification & Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Intent classifier ready\n"
     ]
    }
   ],
   "source": [
    "@observe()\n",
    "def classify_intent(query: str) -> str:\n",
    "    \"\"\"Classify user query into HR, Tech, or Finance.\"\"\"\n",
    "    prompt = f\"\"\"Classify this query into one category: HR, Tech, or Finance.\n",
    "\n",
    "Categories:\n",
    "- HR: Benefits, leave, recruitment, payroll, performance reviews\n",
    "- Tech: IT support, software, hardware, network, troubleshooting\n",
    "- Finance: Expenses, budgets, invoices, accounting, taxes\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Respond with ONLY one word: HR, Tech, or Finance\n",
    "Classification:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    classification = response.content.strip().upper()\n",
    "    \n",
    "    # Normalize response\n",
    "    if \"HR\" in classification:\n",
    "        return \"HR\"\n",
    "    elif \"TECH\" in classification or \"IT\" in classification:\n",
    "        return \"Tech\"\n",
    "    elif \"FINANCE\" in classification:\n",
    "        return \"Finance\"\n",
    "    else:\n",
    "        return \"HR\"  # Default fallback\n",
    "\n",
    "print(\"âœ“ Intent classifier ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Routing function ready\n"
     ]
    }
   ],
   "source": [
    "@observe()\n",
    "def route_to_agent(query: str, intent: str) -> Dict:\n",
    "    \"\"\"Route query to the appropriate specialized agent.\"\"\"\n",
    "    # Agent mapping\n",
    "    agents = {\n",
    "        \"HR\": (hr_chain, hr_retriever),\n",
    "        \"Tech\": (tech_chain, tech_retriever),\n",
    "        \"Finance\": (finance_chain, finance_retriever)\n",
    "    }\n",
    "    \n",
    "    # Get agent (default to HR if intent not found)\n",
    "    chain, retriever = agents.get(intent, agents[\"HR\"])\n",
    "    \n",
    "    # Get answer and source documents\n",
    "    answer = chain.invoke(query)\n",
    "    # In LangChain 1.0+, retrievers use .invoke() instead of .get_relevant_documents()\n",
    "    sources = retriever.invoke(query)\n",
    "    \n",
    "    return {\n",
    "        \"intent\": intent,\n",
    "        \"answer\": answer,\n",
    "        \"sources\": sources\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Routing function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Orchestrator ready\n"
     ]
    }
   ],
   "source": [
    "@observe()\n",
    "def orchestrator(query: str) -> Dict:\n",
    "    \"\"\"Main orchestrator: classify intent and route to appropriate agent.\"\"\"\n",
    "    # Classify intent\n",
    "    intent = classify_intent(query)\n",
    "    \n",
    "    # Route to agent\n",
    "    result = route_to_agent(query, intent)\n",
    "    \n",
    "    # Format response\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"classified_intent\": intent,\n",
    "        \"answer\": result[\"answer\"],\n",
    "        \"sources\": [doc.page_content[:200] + \"...\" for doc in result[\"sources\"][:2]]\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Orchestrator ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Loaded 15 test queries\n"
     ]
    }
   ],
   "source": [
    "# Load test queries\n",
    "try:\n",
    "    with open(\"test_queries.json\", \"r\") as f:\n",
    "        test_queries = json.load(f)\n",
    "    print(f\"âœ“ Loaded {len(test_queries)} test queries\")\n",
    "except FileNotFoundError:\n",
    "    test_queries = [\n",
    "        {\"query\": \"How many vacation days do I get per year?\", \"expected_intent\": \"HR\"},\n",
    "        {\"query\": \"My laptop won't connect to WiFi\", \"expected_intent\": \"Tech\"},\n",
    "        {\"query\": \"What is the expense reimbursement process?\", \"expected_intent\": \"Finance\"}\n",
    "    ]\n",
    "    print(f\"âœ“ Using {len(test_queries)} default test queries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Query: How many vacation days do I get per year?\n",
      "Expected: HR\n",
      "Classified: HR\n",
      "Match: âœ“\n",
      "\n",
      "Answer: You are entitled to 20 days of annual leave per year as a full-time employee....\n",
      "\n",
      "============================================================\n",
      "Query: My laptop won't connect to WiFi\n",
      "Expected: Tech\n",
      "Classified: Tech\n",
      "Match: âœ“\n",
      "\n",
      "Answer: If your laptop won't connect to WiFi, please follow these steps to troubleshoot the issue:\n",
      "\n",
      "1. **Check Signal Strength**: Ensure that you are within range of the WiFi network. Look for the WiFi icon o...\n",
      "\n",
      "============================================================\n",
      "Query: What is the expense reimbursement process?\n",
      "Expected: Finance\n",
      "Classified: Finance\n",
      "Match: âœ“\n",
      "\n",
      "Answer: The expense reimbursement process involves the following steps:\n",
      "\n",
      "1. **Submission Timeline**: Submit your expense report within 30 days of incurring the expense.\n",
      "\n",
      "2. **Expense Management Portal**: Use ...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the expense reimbursement process?',\n",
       " 'classified_intent': 'Finance',\n",
       " 'answer': 'The expense reimbursement process involves the following steps:\\n\\n1. **Submission Timeline**: Submit your expense report within 30 days of incurring the expense.\\n\\n2. **Expense Management Portal**: Use the designated expense management portal to submit your expenses.\\n\\n3. **Receipt Requirements**: \\n   - Original receipts, photos, or PDFs are required for all expenses over $25.\\n   - Receipts must include the date, amount, vendor, and description of the expense.\\n   - If a receipt is missing, an explanation is required.\\n   - Credit card statements alone are not sufficient for reimbursement.\\n   - For expenses in foreign currency, convert the amount to USD with the date of the transaction.\\n\\n4. **Approval Process**:\\n   - Expenses under $100 require manager approval.\\n   - Expenses between $100 and $1,000 require department head approval.\\n   - Expenses over $1,000 require VP approval.\\n   - For urgent requests, expedited approval can be sought.\\n   - If an expense is denied, you can appeal with additional documentation.\\n\\n5. **Processing Time**: Once approved, the reimbursement will be processed within 5-7 business days.\\n\\n6. **Payment Method**: Reimbursement payments will be made via direct deposit to your bank account. \\n\\nEnsure all guidelines and limits for eligible expenses are followed to facilitate a smooth reimbursement process.',\n",
       " 'sources': ['EXPENSE REIMBURSEMENT POLICY\\n\\nGuidelines for submitting and getting reimbursed for business expenses.\\n\\nELIGIBLE EXPENSES\\n- Travel: Flights, hotels, rental cars, meals\\n- Meals: Business meals with clie...',\n",
       "  'EXPENSE REIMBURSEMENT POLICY\\n\\nGuidelines for submitting and getting reimbursed for business expenses.\\n\\nELIGIBLE EXPENSES\\n- Travel: Flights, hotels, rental cars, meals\\n- Meals: Business meals with clie...']}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test individual queries\n",
    "def test_query(query_text: str, expected: str = None):\n",
    "    \"\"\"Test a single query.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Query: {query_text}\")\n",
    "    if expected:\n",
    "        print(f\"Expected: {expected}\")\n",
    "    \n",
    "    result = orchestrator(query_text)\n",
    "    \n",
    "    print(f\"Classified: {result['classified_intent']}\")\n",
    "    if expected:\n",
    "        match = \"âœ“\" if result['classified_intent'].upper() == expected.upper() else \"âœ—\"\n",
    "        print(f\"Match: {match}\")\n",
    "    \n",
    "    print(f\"\\nAnswer: {result['answer'][:200]}...\")\n",
    "    return result\n",
    "\n",
    "# Quick test\n",
    "test_query(\"How many vacation days do I get per year?\", \"HR\")\n",
    "test_query(\"My laptop won't connect to WiFi\", \"Tech\")\n",
    "test_query(\"What is the expense reimbursement process?\", \"Finance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RUNNING ALL TEST QUERIES\n",
      "============================================================\n",
      "\n",
      "[1/15] How many vacation days do I get per year?\n",
      "\n",
      "[2/15] My laptop won't connect to WiFi, what should I do?\n",
      "\n",
      "[3/15] What is the expense reimbursement process?\n",
      "\n",
      "[4/15] How do I reset my password?\n",
      "\n",
      "[5/15] What is our 401(k) matching policy?\n",
      "\n",
      "[6/15] When are invoices processed for payment?\n",
      "\n",
      "[7/15] Can I work remotely?\n",
      "\n",
      "[8/15] How do I access the company VPN?\n",
      "\n",
      "[9/15] What is the budget approval process?\n",
      "\n",
      "[10/15] How do I request time off?\n",
      "\n",
      "[11/15] What software tools are available for project management?\n",
      "\n",
      "[12/15] How do I submit an expense report?\n",
      "\n",
      "[13/15] What is the performance review process?\n",
      "\n",
      "[14/15] My email is not working, who should I contact?\n",
      "\n",
      "[15/15] What are the purchase approval limits?\n",
      "\n",
      "============================================================\n",
      "TEST SUMMARY\n",
      "============================================================\n",
      "Correct: 15/15\n",
      "Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Run all test queries\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RUNNING ALL TEST QUERIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "results = []\n",
    "for i, test_case in enumerate(test_queries, 1):\n",
    "    query = test_case.get(\"query\", test_case)\n",
    "    expected = test_case.get(\"expected_intent\") if isinstance(test_case, dict) else None\n",
    "    \n",
    "    print(f\"\\n[{i}/{len(test_queries)}] {query}\")\n",
    "    result = orchestrator(query)\n",
    "    \n",
    "    correct = expected is None or result['classified_intent'].upper() == expected.upper()\n",
    "    results.append({\"query\": query, \"expected\": expected, \"classified\": result['classified_intent'], \"correct\": correct})\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "correct_count = sum(1 for r in results if r['correct'] and r['expected'])\n",
    "total_with_expected = sum(1 for r in results if r['expected'])\n",
    "if total_with_expected > 0:\n",
    "    accuracy = (correct_count / total_with_expected) * 100\n",
    "    print(f\"Correct: {correct_count}/{total_with_expected}\")\n",
    "    print(f\"Accuracy: {accuracy:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Langfuse Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Langfuse: Connected\n",
      "âœ“ Traces flushed\n",
      "\n",
      "ðŸ“Š View traces at: https://cloud.langfuse.com\n"
     ]
    }
   ],
   "source": [
    "# Verify Langfuse connection and flush traces\n",
    "try:\n",
    "    connected = langfuse.auth_check()\n",
    "    print(f\"âœ“ Langfuse: {'Connected' if connected else 'Not connected'}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš  Langfuse error: {e}\")\n",
    "\n",
    "# Flush traces\n",
    "langfuse.flush()\n",
    "print(\"âœ“ Traces flushed\")\n",
    "print(f\"\\nðŸ“Š View traces at: {os.getenv('LANGFUSE_HOST', 'https://cloud.langfuse.com')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluator Agent (Bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Evaluator ready\n"
     ]
    }
   ],
   "source": [
    "@observe()\n",
    "def evaluate_response(query: str, answer: str, trace_id: str = None) -> Dict:\n",
    "    \"\"\"Evaluate response quality on multiple dimensions.\"\"\"\n",
    "    import re\n",
    "    \n",
    "    prompt = f\"\"\"Evaluate this Q&A pair and provide scores (1-10) for each dimension.\n",
    "\n",
    "Query: {query}\n",
    "Answer: {answer}\n",
    "\n",
    "Dimensions:\n",
    "1. Relevance: Does the answer address the query?\n",
    "2. Completeness: Is the answer complete?\n",
    "3. Accuracy: Is the answer factually correct?\n",
    "4. Clarity: Is the answer clear and well-structured?\n",
    "\n",
    "Provide scores as JSON: {{\"relevance\": X, \"completeness\": X, \"accuracy\": X, \"clarity\": X}}\n",
    "JSON:\"\"\"\n",
    "    \n",
    "    response = llm.invoke(prompt)\n",
    "    content = response.content.strip()\n",
    "    \n",
    "    # Parse JSON from response\n",
    "    json_match = re.search(r'\\{[^}]+\\}', content)\n",
    "    if json_match:\n",
    "        try:\n",
    "            scores = json.loads(json_match.group())\n",
    "        except:\n",
    "            scores = {\"relevance\": 7, \"completeness\": 7, \"accuracy\": 7, \"clarity\": 7}\n",
    "    else:\n",
    "        # Fallback: extract numbers\n",
    "        numbers = re.findall(r'\\d+', content)\n",
    "        scores = {\n",
    "            \"relevance\": int(numbers[0]) if len(numbers) > 0 else 7,\n",
    "            \"completeness\": int(numbers[1]) if len(numbers) > 1 else 7,\n",
    "            \"accuracy\": int(numbers[2]) if len(numbers) > 2 else 7,\n",
    "            \"clarity\": int(numbers[3]) if len(numbers) > 3 else 7\n",
    "        }\n",
    "    \n",
    "    overall = sum(scores.values()) / len(scores)\n",
    "    \n",
    "    # Record in Langfuse if trace_id provided\n",
    "    if trace_id:\n",
    "        try:\n",
    "            for dim, score in scores.items():\n",
    "                langfuse.score(name=f\"eval_{dim}\", value=score, trace_id=trace_id)\n",
    "            langfuse.score(name=\"eval_overall\", value=overall, trace_id=trace_id)\n",
    "        except Exception as e:\n",
    "            pass  # Skip if Langfuse recording fails\n",
    "    \n",
    "    return {\"scores\": scores, \"overall_score\": round(overall, 2)}\n",
    "\n",
    "print(\"âœ“ Evaluator ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How many vacation days do I get per year?\n",
      "Intent: HR\n",
      "\n",
      "Answer: You are entitled to 20 days of annual leave per year as a full-time employee....\n",
      "\n",
      "Evaluation:\n",
      "  relevance: 10/10\n",
      "  completeness: 10/10\n",
      "  accuracy: 10/10\n",
      "  clarity: 10/10\n",
      "  Overall: 10.0/10\n"
     ]
    }
   ],
   "source": [
    "@observe()\n",
    "def orchestrator_with_evaluation(query: str) -> Dict:\n",
    "    \"\"\"Orchestrator with automatic quality evaluation.\"\"\"\n",
    "    # Get trace ID (optional - evaluation works without it)\n",
    "    # Note: trace_id can be passed manually if needed for Langfuse scoring\n",
    "    trace_id = None  # Simplified - evaluation will work without trace_id\n",
    "    \n",
    "    # Classify and route\n",
    "    intent = classify_intent(query)\n",
    "    result = route_to_agent(query, intent)\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluation = evaluate_response(query, result[\"answer\"], trace_id)\n",
    "    \n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"classified_intent\": intent,\n",
    "        \"answer\": result[\"answer\"],\n",
    "        \"sources\": [doc.page_content[:200] + \"...\" for doc in result[\"sources\"][:2]],\n",
    "        \"evaluation\": evaluation\n",
    "    }\n",
    "\n",
    "# Test evaluator\n",
    "result = orchestrator_with_evaluation(\"How many vacation days do I get per year?\")\n",
    "print(f\"Query: {result['query']}\")\n",
    "print(f\"Intent: {result['classified_intent']}\")\n",
    "print(f\"\\nAnswer: {result['answer'][:200]}...\")\n",
    "print(f\"\\nEvaluation:\")\n",
    "for dim, score in result['evaluation']['scores'].items():\n",
    "    print(f\"  {dim}: {score}/10\")\n",
    "print(f\"  Overall: {result['evaluation']['overall_score']}/10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
